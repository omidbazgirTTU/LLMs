{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMbMdmUf/gqkZ0v+bFzD3/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "43001b6ac5f744058f6a002f2a29c806": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1058081b9fe6481a9044ce2fc6f551cb",
              "IPY_MODEL_1411546a1aa74dd492ee90b9ac28496e",
              "IPY_MODEL_396f1f7bee0a4749a80458f9ce30a157"
            ],
            "layout": "IPY_MODEL_31b47d5e6a7b4b7087ee7bf781d39467"
          }
        },
        "1058081b9fe6481a9044ce2fc6f551cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef172828b1904d2d8034ea5d3d6baa7a",
            "placeholder": "​",
            "style": "IPY_MODEL_5d364d25e030459b88ca93f3905d3bbe",
            "value": "  0%"
          }
        },
        "1411546a1aa74dd492ee90b9ac28496e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf5e8a2a87ca496b898d2ccdb22961b8",
            "max": 488,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_780e45eafbf248d989d951daaff23a3c",
            "value": 0
          }
        },
        "396f1f7bee0a4749a80458f9ce30a157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a31d983a6839483db10b2e9b711df244",
            "placeholder": "​",
            "style": "IPY_MODEL_76b93db498d349a1bccb15b060e50eda",
            "value": " 0/488 [00:00&lt;?, ?it/s]"
          }
        },
        "31b47d5e6a7b4b7087ee7bf781d39467": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef172828b1904d2d8034ea5d3d6baa7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d364d25e030459b88ca93f3905d3bbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf5e8a2a87ca496b898d2ccdb22961b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "780e45eafbf248d989d951daaff23a3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a31d983a6839483db10b2e9b711df244": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76b93db498d349a1bccb15b060e50eda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omidbazgirTTU/LLMs/blob/main/PPO_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proximal Policy Implementation (PPO)\n",
        "## Author Omid Bazgir\n",
        "## Thanks for the great tutorial by Ehsan Kamalinejad (EK)\n",
        "### Linke to the tutorial on YouTube https://www.youtube.com/watch?v=3uvnoVjM8nY&list=PLb9xatikqn0fwsS-Le1mkyQ2uZzK8DeP1&index=3\n",
        "\n",
        "### Basic implementation of PPO (this example is for cart pole program) which is a reinforcement learning (RL) algorithm with many applications including game design, development, large language models (LLMs). PPO is being used in LLMs as as finetuning technique through reinforcement learning with human feedback (RLHF).\n",
        "\n",
        "### more details on the cart pole problem including the how to define the reward values, action space and so on is provided in the Gymnasium documentation https://www.gymlibrary.dev/environments/classic_control/cart_pole/\n",
        "\n",
        "References introduced by EK:\n",
        "- [EK's Video Lecture](https://www.youtube.com/watch?v=3uvnoVjM8nY) This is the lecture where we did a deep dive into the theory of PPO.\n",
        "- [OpenAI PPO Repo](https://github.com/openai/baselines/blob/master/baselines/ppo2/runner.py) This is helpful as a reference for further implementations.\n",
        "- [PPO Paper](https://arxiv.org/abs/1707.06347) This is the original paper that introduced PPO.\n",
        "- [Sergey Levine UC Berkley CS285](http://rail.eecs.berkeley.edu/deeprlcourse/) This is a complete course in RL.\n",
        "- [Pieter Abbeel mini-course](https://www.youtube.com/watch?v=2GwBez0D20A&list=PLwRJQ4m4UJjNymuBM9RdmB3Z9N5-0IlY0) This is a mini-course focusing on TRPO, PPO, DDPG and model free RL.\n",
        "- [OpenAI Documentation on RL](https://spinningup.openai.com/en/latest/index.html) THis is OpenAI documentation on RL and parts of our code was borrowed from here.\n",
        "- [labml.ai](https://nn.labml.ai/) This repo contains popular papers with their annotated PyTorch implementations.\n",
        "- [cleanrl](https://github.com/vwxyzjn/cleanrl) This repo has clean implementations of RL algorithms and parts of our code was borrowed from here.\n",
        "\n"
      ],
      "metadata": {
        "id": "i-CL8MeIeT3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installing dependencies\n",
        "!pip install torch --extra-index-url https://download.pytorch.org/whl/cu116\n",
        "!pip install moviepy omegaconf matplotlib\n",
        "!pip install gym==0.26.2\n",
        "!pip install git+https://github.com/carlosluis/stable-baselines3@fix_tests\n",
        "!pip install gym[classic_control] gym[atari] gym[accept-rom-license] gym[other]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5RDbhrg5d9U_",
        "outputId": "87ce8461-61b2-4b6d-83f1-846351e94a83"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Collecting omegaconf\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.0)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.23.5)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.6)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.4.9)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf) (6.0.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy) (67.7.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2023.11.17)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=1b9361442a3188671fbfe6416f48cc1b2a945a115ee3d6a849764f9a70966a08\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, omegaconf\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 omegaconf-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym==0.26.2\n",
            "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.26.2) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.26.2) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.26.2) (0.0.8)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827620 sha256=2fbe11ecc1f57d257aebbadb5ce4ed3f13f7e963c3d4f1fdd1c035ba539605e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/22/6d/3e7b32d98451b4cd9d12417052affbeeeea012955d437da1da\n",
            "Successfully built gym\n",
            "Installing collected packages: gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.0.6 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gym-0.26.2\n",
            "Collecting git+https://github.com/carlosluis/stable-baselines3@fix_tests\n",
            "  Cloning https://github.com/carlosluis/stable-baselines3 (to revision fix_tests) to /tmp/pip-req-build-5r9vro91\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/carlosluis/stable-baselines3 /tmp/pip-req-build-5r9vro91\n",
            "  Running command git checkout -b fix_tests --track origin/fix_tests\n",
            "  Switched to a new branch 'fix_tests'\n",
            "  Branch 'fix_tests' set up to track remote branch 'fix_tests' from 'origin'.\n",
            "  Resolved https://github.com/carlosluis/stable-baselines3 to commit 6617e6e73cb3a70f3e88cea780ea12bed95c099e\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym==0.26.2 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.0.0a0) (0.26.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.0.0a0) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.0.0a0) (2.1.0+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.0.0a0) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.0.0a0) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.0.0a0) (3.7.1)\n",
            "Collecting importlib-metadata~=4.13 (from stable-baselines3==2.0.0a0)\n",
            "  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.26.2->stable-baselines3==2.0.0a0) (0.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata~=4.13->stable-baselines3==2.0.0a0) (3.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3==2.0.0a0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3==2.0.0a0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3==2.0.0a0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3==2.0.0a0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3==2.0.0a0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3==2.0.0a0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->stable-baselines3==2.0.0a0) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a0) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a0) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a0) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a0) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a0) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a0) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3==2.0.0a0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3==2.0.0a0) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3==2.0.0a0) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11->stable-baselines3==2.0.0a0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11->stable-baselines3==2.0.0a0) (1.3.0)\n",
            "Building wheels for collected packages: stable-baselines3\n",
            "  Building wheel for stable-baselines3 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stable-baselines3: filename=stable_baselines3-2.0.0a0-py3-none-any.whl size=174557 sha256=667b27921ee04f7f15cfb1ccf7eef6d3daf6334786ab5628ddb91097ea427eee\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2b9ivmdn/wheels/cc/40/56/e6db2f8ff9427b0849f4c6ddbe003a53a5f61f31aae2f9ccb7\n",
            "Successfully built stable-baselines3\n",
            "Installing collected packages: importlib-metadata, stable-baselines3\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 7.0.0\n",
            "    Uninstalling importlib-metadata-7.0.0:\n",
            "      Successfully uninstalled importlib-metadata-7.0.0\n",
            "Successfully installed importlib-metadata-4.13.0 stable-baselines3-2.0.0a0\n",
            "Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.10/dist-packages (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (0.0.8)\n",
            "Collecting pygame==2.1.0 (from gym[classic_control])\n",
            "  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ale-py~=0.8.0 (from gym[classic_control])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2 (from gym[classic_control])\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting lz4>=3.1.0 (from gym[classic_control])\n",
            "  Downloading lz4-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python>=3.0 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (4.8.0.76)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (3.7.1)\n",
            "Requirement already satisfied: moviepy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (1.0.3)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.0->gym[classic_control]) (6.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.0->gym[classic_control]) (4.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[classic_control]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[classic_control]) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[classic_control]) (4.66.1)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gym[classic_control])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym[classic_control]) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym[classic_control]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym[classic_control]) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym[classic_control]) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym[classic_control]) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym[classic_control]) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym[classic_control]) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym[classic_control]) (2.8.2)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gym[classic_control]) (4.4.2)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gym[classic_control]) (0.1.10)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gym[classic_control]) (2.31.6)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gym[classic_control]) (0.4.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy>=1.0.0->gym[classic_control]) (67.7.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->gym[classic_control]) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[classic_control]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[classic_control]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[classic_control]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[classic_control]) (2023.11.17)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=f47ba8072d4599f118fb8cac0bf83bcc48dff19bc565308672853f2345be0ce2\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: pygame, lz4, ale-py, AutoROM.accept-rom-license, autorom\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.5.2\n",
            "    Uninstalling pygame-2.5.2:\n",
            "      Successfully uninstalled pygame-2.5.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.0.6 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.4.2 lz4-4.3.2 pygame-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "plt.style.use('dark_background')\n",
        "from tqdm.notebook import tqdm\n",
        "from omegaconf import DictConfig\n",
        "\n",
        "import gym\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.distributions.categorical import Categorical\n",
        "\n",
        "from IPython.display import Video"
      ],
      "metadata": {
        "id": "vMQjpnItf6A0"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up\n",
        "seed = 7\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n"
      ],
      "metadata": {
        "id": "-kZgZruCgOBP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "93RQAMK5gpXZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "configs = {\n",
        "    # experiment arguments\n",
        "    \"exp_name\": \"cartpole\",\n",
        "    \"gym_id\": \"CartPole-v1\", # the id of from OpenAI gym\n",
        "    # training arguments\n",
        "    \"learning_rate\": 1e-3, # the learning rate of the optimizer\n",
        "    \"total_timesteps\": 1000000, # total timesteps of the training\n",
        "    \"max_grad_norm\": 0.5, # the maximum norm allowed for the gradient\n",
        "    # PPO parameters\n",
        "    \"num_trajcts\": 32, # N\n",
        "    \"max_trajects_length\": 64, # T\n",
        "    \"gamma\": 0.99, # gamma\n",
        "    \"gae_lambda\":0.95, # lambda for the generalized advantage estimation\n",
        "    \"num_minibatches\": 2, # number of mibibatches used in each gradient\n",
        "    \"update_epochs\": 2, # number of full rollout storage creations\n",
        "    \"clip_epsilon\": 0.2, # the surrogate clipping coefficient\n",
        "    \"ent_coef\": 0.01, # entroy coefficient controlling the exploration factor C2\n",
        "    \"vf_coef\": 0.5, # value function controlling value estimation importance C1\n",
        "    # visualization and print parameters\n",
        "    \"num_returns_to_average\": 3, # how many episodes to use for printing average return\n",
        "    \"num_episodes_to_average\": 23, # how many episodes to use for smoothing of the return diagram\n",
        "    }\n",
        "\n",
        "# batch_size is the size of the flatten sequences when trajcts are flatten\n",
        "configs['batch_size'] = int(configs['num_trajcts'] * configs['max_trajects_length'])\n",
        "# number of samples used in each gradient\n",
        "configs['minibatch_size'] = int(configs['batch_size'] // configs['num_minibatches'])\n",
        "\n",
        "configs = DictConfig(configs)\n",
        "\n",
        "run_name = f\"{configs.gym_id}__{configs.exp_name}__{seed}__{int(time.time())}\""
      ],
      "metadata": {
        "id": "XEC7N3Dmgy_m"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ENV\n",
        "`envs` us set of parallel environments each holding a random initiali `state` and accepts an `action` to change and return its new state."
      ],
      "metadata": {
        "id": "WWyv6MMjh-PH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating an env with random state\n",
        "def make_env_func(gym_id, seed, idx, run_name, capture_video = False):\n",
        "  def env_fun():\n",
        "    env = gym.make(gym_id, render_mode = \"rgb_gray\")\n",
        "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "    if capture_video:\n",
        "      # initiate the video capture if not already initiated\n",
        "      if idx ==0:\n",
        "        #wrapper to create the video of the performance\n",
        "        env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
        "    env.action_space.seed(seed)\n",
        "    env.observation_space.seed(seed)\n",
        "\n",
        "    return env\n",
        "  return env_fun"
      ],
      "metadata": {
        "id": "_M6xogrBiU88"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create N (here is 32) parallel envs\n",
        "envs = []\n",
        "for i in range(configs.num_trajcts):\n",
        "  envs.append(make_env_func(configs.gym_id, seed+i, i, run_name))\n",
        "envs = gym.vector.SyncVectorEnv(envs)\n",
        "envs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsTSdYPqjqwn",
        "outputId": "489beaab-246e-4c59-f0cf-8a507eb9af05"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:623: UserWarning: \u001b[33mWARN: The environment is being initialised with mode (rgb_gray) that is not in the possible render_modes (['human', 'rgb_array']).\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SyncVectorEnv(32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "A simple MLP (or fully connected layers FC) model that gets a state and has two methods:\n",
        "* `agent.value_func(state)` gets a state and returns the estimated expected total feature rewards from that state $V_{\\theta}(s)$.\n",
        "\n",
        "* `agent.policy(state)` gets a state and returns next `action`, `log_prob` of actions, the `entropy` and `value`."
      ],
      "metadata": {
        "id": "VhAEm15CkF6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FCBlock(nn.Module):\n",
        "    \"\"\"A generic fully connected residual block with good setup\"\"\"\n",
        "    def __init__(self, embed_dim, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.LayerNorm(embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(embed_dim, 4*embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4*embed_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)\n",
        "\n",
        "class Agent(nn.Module):\n",
        "  \"\"\" an agent that creates actions and estimates values\"\"\"\n",
        "  def __init__(self, env_observation_dim, action_space_dim, embed_dim = 64, num_blocks=2):\n",
        "    super().__init__()\n",
        "    # getting the observation and embed that into another space `embed_dim`\n",
        "    self.embedding_layer = nn.Linear(env_observation_dim, embed_dim)\n",
        "    # layers that are shared between policy head and value head\n",
        "    # it not necessarily needed to have a shared layer, but here since that value and policy tasks are quite similar\n",
        "    # we can use several shared layer to do multi-task learning\n",
        "    self.shared_layers = nn.Sequential(*[FCBlock(embed_dim=embed_dim) for _ in range(num_blocks)])\n",
        "    self.value_head = nn.Linear(embed_dim, 1)\n",
        "    self.policy_head = nn.Linear(embed_dim, action_space_dim)\n",
        "    # orthogonal initialization with a hi entropy for exploration at the start\n",
        "    torch.nn.init.orthogonal_(self.policy_head.weight, 0.01)\n",
        "\n",
        "  def value_func(self,state):\n",
        "    hidden = self.shared_layers(self.embedding_layer(state))\n",
        "    value = self.value_head(hidden)\n",
        "    return value\n",
        "\n",
        "  def policy(self, state, action=None):\n",
        "    # plicy is supposed to create actions but here it takes actions as the input\n",
        "    # this is for phase 2 of PPO where we want to analze the actions\n",
        "    hidden = self.shared_layers(self.embedding_layer(state))\n",
        "    logits = self.policy_head(hidden)\n",
        "    # Pytoch categorical class for sampling and probability calcucation\n",
        "    probs = Categorical(logits=logits)\n",
        "    if action is None:\n",
        "      action = probs.sample()\n",
        "    return action, probs.log_prob(action), probs.entropy(), self.value_head(hidden)"
      ],
      "metadata": {
        "id": "WpUuRBcTk38V"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generalized Advantage Estimation (GAE)\n",
        "\n",
        "### $Advantage (s, a)$ calculates how better or worse the return of taking the action $a$ at the state $s$ is compared to expected return for all other actions in that state.\n",
        "\n",
        "### we can approximate that with the below reverse formulas\n",
        "\n",
        "$δ_{t} = r_{t} + γV(s_{t+1}) - V(s_{t})$\n",
        "\n",
        "$\\hat{A_{t}} = δ_{t} + γλ\\hat{A}_{t+1}$\n"
      ],
      "metadata": {
        "id": "Xk-6HbWcrwJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gae(\n",
        "    cur_observations,   # the current state when advantages will be calculated\n",
        "    rewards,            # rewards collected from trajectories of shape [num_trajcts, max_trajcts_length]\n",
        "    dones,              # binary marker of end of trajectories of shape [num_trajcts, max_trajcts_length]\n",
        "    values,             # value estimates collected over trajectories of shape [num_trajcts, max_trajcts_length]\n",
        "):\n",
        "\n",
        "  advantages = torch.zeros((configs.num_trajcts, configs.max_trajects_length))\n",
        "  last_advantage = 0\n",
        "\n",
        "  # the value after the last step\n",
        "  with torch.no_grad():\n",
        "    last_value = agent.value_func(cur_observations).reshape(1,-1)\n",
        "\n",
        "  # reverse recursive to calculate advantages based on the delta formula\n",
        "  for t in reversed(range(configs.max_trajects_length)):\n",
        "    # mask if episode completed after step t\n",
        "    mask = 1.0 - dones[:,t] # --> if we are looking for those trajectories that were ended quicker we don't need to any further calculation so we use the variable mask\n",
        "    last_value = last_value * mask\n",
        "    last_advantage = last_advantage * mask\n",
        "    delta = rewards[:,t] + configs.gamma * last_value - values[:,t]\n",
        "    last_advantage = delta + configs.gamma * configs.gae_lambda * last_advantage\n",
        "    advantages[:,t] = last_advantage\n",
        "    last_value = values[:,t]\n",
        "\n",
        "  advantages = advantages.to(device)\n",
        "  returns = advantages + values\n",
        "\n",
        "  return advantages, returns"
      ],
      "metadata": {
        "id": "-IPXHfFmz8rT"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Rollout stage\n",
        "\n",
        "### Phase 1: rollout creation\n",
        "\n",
        "1. Generate $N$ trajectories of length $T$ by $\\pi_{θ_{old}}$\n",
        "2. calculate $logits_{old}$ for the actions\n",
        "3. calculate $V_{theta}$ along the trajectories\n",
        "4. calculate advantage estimates $\\hat{A}_{t}$\n",
        "5. create a storage and add all items to it"
      ],
      "metadata": {
        "id": "CvRwdFGi2stp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_rollout(\n",
        "    envs,               # parallel envs creating trajectories\n",
        "    cur_observation,    # starting observation of shape [num_trajcts, observation_dim]\n",
        "    cur_done,           # current termination status of shape [num_trajcts,]\n",
        "    all_returns         # a list to track returns\n",
        "):\n",
        "  \"\"\"\n",
        "  rollout phase: create parallel trajectories and store them in the rolout storage\n",
        "  \"\"\"\n",
        "\n",
        "  # cache empty tensors to store the rollouts\n",
        "  observations = torch.zeros((configs.num_trajcts, configs.max_trajects_length) +\n",
        "                             envs.single_observation_space.shape).to(device)\n",
        "  actions = torch.zeros((configs.num_trajcts, configs.max_trajects_length) +\n",
        "                        envs.single_action_space.shape).to(device)\n",
        "  logprobs = torch.zeros((configs.num_trajcts, configs.max_trajects_length)).to(device)\n",
        "  rewards = torch.zeros((configs.num_trajcts, configs.max_trajects_length)).to(device)\n",
        "  dones = torch.zeros((configs.num_trajcts, configs.max_trajects_length)).to(device)\n",
        "  values = torch.zeros((configs.num_trajcts, configs.max_trajects_length)).to(device)\n",
        "\n",
        "  for t in range(configs.max_trajects_length):\n",
        "    observations[:,t] = cur_observation\n",
        "    dones[:,t] = cur_done\n",
        "\n",
        "    # give observation to the model and collect action, logprobs of actions, entropy and value\n",
        "    with torch.no_grad():\n",
        "      action, logprob, entropy, value = agent.policy(cur_observation)\n",
        "\n",
        "    values[:,t] = value.flatten()\n",
        "    actions[:,t] = action\n",
        "    logprobs[:,t] = logprob\n",
        "\n",
        "    # apply the action to the env and collect observation and reward\n",
        "    cur_observation, reward, cur_done, _, info = envs.step(action.cpu().numpy())\n",
        "    rewards[:,t] = torch.Tensor(reward).to(device).view(-1)\n",
        "    cur_observation = torch.Tensor(cur_observation).to(device)\n",
        "    cur_done = torch.Tensor(cur_done).to(device)\n",
        "\n",
        "    # if an episode ended store its total reward for progress report\n",
        "    if info:\n",
        "      for item in info['final_info']:\n",
        "        if item and \"episode\" in item.keys():\n",
        "          all_returns.append(item['episode']['r'])\n",
        "          break\n",
        "\n",
        "    # create rollout storage\n",
        "    rollout = {\n",
        "        'cur_observation': cur_observation,\n",
        "        'cur_done': cur_done,\n",
        "        'observations': observations,\n",
        "        'actions': actions,\n",
        "        'logprobs' : logprobs,\n",
        "        'values' : values,\n",
        "        'dones' : dones,\n",
        "        'rewards' : rewards\n",
        "    }\n",
        "\n",
        "    return rollout\n"
      ],
      "metadata": {
        "id": "1oQ2TX1g30cW"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a standard pytorch dataset to store the RL model output in a standard dataset to be used for downstream tasks (NLP, LLM)\n",
        "class Storage(Dataset):\n",
        "  def __init__(self, rollout, advantages, returns, envs):\n",
        "    # fill in the storage and flatten the parallel trajectories\n",
        "    self.observations = rollout['observations'].reshape((-1,) + envs.single_observation_space.shape)\n",
        "    self.logprobs = rollout['logprobs'].reshape(-1)\n",
        "    self.actions = rollout['actions'].reshape((-1,) + envs.single_observation_space.shape).long()\n",
        "    self.advantages = advantages.reshape(-1)\n",
        "    self.returns = returns.reshape(-1)\n",
        "\n",
        "\n",
        "  def __getitem__(self, ix: int):\n",
        "    item = [\n",
        "        self.observations[ix],\n",
        "        self.logprobs[ix],\n",
        "        self.actions[ix],\n",
        "        self.advantages[ix],\n",
        "        self.returns[ix],\n",
        "    ]\n",
        "    return item\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    return len(self.observations)"
      ],
      "metadata": {
        "id": "jr0-Yc2KvMGN"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Functions\n",
        "\n",
        "$L^{CLIP} = min(\\frac{\\pi_{theta}(a_{t}|s_{t})}{\\pi_{theta_{old}}(a_{t}|s_{t})}.\\hat{A_{t}},\n",
        "clip(\\frac{\\pi_{theta}(a_{t}|s_{t})}{\\pi_{theta_{old}}(a_{t}|s_{t})},1-ϵ, 1+ϵ).\\hat{A_{t}})$\n",
        "\n",
        "$L^{VF} = \\frac{1}{2} \\left \\lVert V_{\\theta}(s) - (Σ_{t=0}^{T} γ^{t}r_{t}|s_{0}=s) \\right \\rVert_2^2 $\n",
        "\n",
        "$L^{ENT} = entropy(\\pi_{θ}(.|s_{t}))$"
      ],
      "metadata": {
        "id": "TE-JCM3PxT-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_clip(\n",
        "    mb_oldlogprob,   # old logprob of mini batch actions collected during the rollout\n",
        "    mb_newlogprob,   # new logprob of mini batch actions created by the new policy\n",
        "    mb_advantages,   # mini batch of advantages collected during the rolout\n",
        "\n",
        "):\n",
        "  \"\"\"\n",
        "  policy loss with clipping to control gradients\n",
        "  \"\"\"\n",
        "\n",
        "  ratio = torch.exp(mb_newlogprob - mb_oldlogprob)\n",
        "  ploicy_loss = -mb_advantages * ratio          # since here we want to maximize the reward and pytorch optimizer always look for minimum value, we convert the sign by multiplying the negative -1\n",
        "  # clipped policy gradient loss enforces closeness\n",
        "  clipped_loss = -mb_advantages * torch.clamp(ratio, 1 - configs.clip_epsilon, 1 + configs.clip_epsilon)\n",
        "  pessimistic_loss = torch.max(ploicy_loss, clipped_loss).mean()\n",
        "  return pessimistic_loss\n",
        "\n",
        "def loss_vf(\n",
        "    mb_oldreturns,    # mini batch of old returns collected during the rollout\n",
        "    mb_newvalues,     # mini batch of values calculated by the new value function\n",
        "):\n",
        "  \"\"\"\n",
        "  enforcing the value function to give more accurate estimates of returns\n",
        "  \"\"\"\n",
        "  mb_newvalues = mb_newvalues.view(-1)\n",
        "  loss = 0.5 * ((mb_newvalues - mb_oldreturns)**2).mean()\n",
        "  return loss"
      ],
      "metadata": {
        "id": "Mam7LR_KypgZ"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n",
        "## The training loop has two phases, phase I and phase II\n",
        "\n",
        "### Phase I: rollout creation\n",
        "\n",
        "1. Generate $N$ trajectories of length $T$ by $\\pi_{θ_{old}}$\n",
        "2. calculate $logits_{old}$ for the actions\n",
        "3. calculate $V_{theta}$ along the trajectories\n",
        "4. calculate advantage estimates $\\hat{A}_{t}$\n",
        "5. create a storage and add all items to it\n",
        "\n",
        "### Phase II: model update\n",
        "\n",
        "- for k epochs\n",
        "  1. select a minibatch of size $ M \\le N*T $ from the storage\n",
        "  2. Generate $logits$ along the minibatch with $\\pi_{\\theta}$\n",
        "  3. Calculate average of $L^{SUR} = L^{CLIP} + c_{1}*L_{VF} + c_{2}*L^{ENT}$\n",
        "- Update $\\theta_{old} <--  \\theta$"
      ],
      "metadata": {
        "id": "A4LdxEis3vlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(\n",
        "    env_observation_dim=envs.single_observation_space.shape[0],\n",
        "    action_space_dim=envs.single_action_space.n\n",
        ").to(device)\n",
        "\n",
        "optimizer = optim.Adam(agent.parameters(), lr=configs.learning_rate)"
      ],
      "metadata": {
        "id": "6EaxF0GU5Li3"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# track returns\n",
        "all_returns = []\n",
        "\n",
        "# initialize the game\n",
        "cur_observation = torch.Tensor(envs.reset()[0]).to(device)\n",
        "cur_done = torch.zeros(configs.num_trajcts).to(device)\n",
        "\n",
        "# progress bar\n",
        "num_updates = configs.total_timesteps // configs.batch_size\n",
        "progress_bar = tqdm(total=num_updates)\n",
        "\n",
        "for update in range(1, num_updates + 1):\n",
        "\n",
        "\n",
        "    ##############################################\n",
        "    # Phase 1: rollout creation\n",
        "\n",
        "    # parallel envs creating trajectories\n",
        "    rollout = create_rollout(envs, cur_observation, cur_done, all_returns)\n",
        "\n",
        "    cur_done = rollout['cur_done']\n",
        "    cur_observation = rollout['cur_observation']\n",
        "    rewards = rollout['rewards']\n",
        "    dones = rollout['dones']\n",
        "    values = rollout['values']\n",
        "\n",
        "    # calculating advantages\n",
        "    advantages, returns = gae(cur_observation, rewards, dones, values)\n",
        "\n",
        "    # a dataset containing the rollouts\n",
        "    dataset = Storage(rollout, advantages, returns, envs)\n",
        "\n",
        "    # a standard dataloader made out of current storage\n",
        "    trainloader = DataLoader(dataset, batch_size=configs.minibatch_size, shuffle=True)\n",
        "\n",
        "\n",
        "    ##############################################\n",
        "    # Phase 2: model update\n",
        "\n",
        "    # linearly shrink the lr from the initial lr to zero\n",
        "    frac = 1.0 - (update - 1.0) / num_updates\n",
        "    optimizer.param_groups[0][\"lr\"] = frac * configs.learning_rate\n",
        "\n",
        "    # training loop\n",
        "    for epoch in range(configs.update_epochs):\n",
        "        for batch in trainloader:\n",
        "            mb_observations, mb_logprobs, mb_actions, mb_advantages, mb_returns = batch\n",
        "\n",
        "            # we calculate the distribution of actions through the updated model revisiting the old trajectories\n",
        "            _, mb_newlogprob, mb_entropy, mb_newvalues = agent.policy(mb_observations, mb_actions)\n",
        "\n",
        "            policy_loss = loss_clip(mb_logprobs, mb_newlogprob, mb_advantages)\n",
        "\n",
        "            value_loss = loss_vf(mb_returns, mb_newvalues)\n",
        "\n",
        "            # average entory of the action space\n",
        "            entropy_loss = mb_entropy.mean()\n",
        "\n",
        "            # full weighted loss\n",
        "            loss = policy_loss - configs.ent_coef * entropy_loss + configs.vf_coef * value_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            # extra clipping of the gradients to avoid overshoots\n",
        "            nn.utils.clip_grad_norm_(agent.parameters(), configs.max_grad_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "    # progress bar\n",
        "    if len(all_returns) > configs.num_returns_to_average:\n",
        "        progress_bar.set_description(f\"episode return: {np.mean(all_returns[-configs.num_returns_to_average:]):.2f}\")\n",
        "        progress_bar.refresh()\n",
        "        progress_bar.update()\n",
        "\n",
        "envs.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431,
          "referenced_widgets": [
            "43001b6ac5f744058f6a002f2a29c806",
            "1058081b9fe6481a9044ce2fc6f551cb",
            "1411546a1aa74dd492ee90b9ac28496e",
            "396f1f7bee0a4749a80458f9ce30a157",
            "31b47d5e6a7b4b7087ee7bf781d39467",
            "ef172828b1904d2d8034ea5d3d6baa7a",
            "5d364d25e030459b88ca93f3905d3bbe",
            "bf5e8a2a87ca496b898d2ccdb22961b8",
            "780e45eafbf248d989d951daaff23a3c",
            "a31d983a6839483db10b2e9b711df244",
            "76b93db498d349a1bccb15b060e50eda"
          ]
        },
        "id": "pHSXBKjK6QcX",
        "outputId": "c4b06a2c-115e-48e8-cc48-9cb0dc971e19"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/488 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43001b6ac5f744058f6a002f2a29c806"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-b398ad4803e3>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mmb_observations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb_logprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb_advantages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-2b6e0f5257c0>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, ix)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvantages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 1732 is out of bounds for dimension 0 with size 512"
          ]
        }
      ]
    }
  ]
}